# -*- coding: utf-8 -*-
"""Summarization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18BXjgyMxZJfRmJO1ou6kEAiUhgsZ6suR
"""

!pip install langchain
!pip install openai
!pip install pinecone-client
!pip install tiktoken
!pip install python-dotenv
!pip install docx2xt
!pip install pypdf
!pip install pandas
!pip install numpy
!pip install langchain_openai
!pip install langchain_community
!pip install wikipedia

import os
os.environ['OPENAI_API_KEY'] = 'YOUR-API-KEY'

"""# **A) Basic Prompt**"""

from langchain_openai import ChatOpenAI
from langchain.schema import(
    AIMessage,
    HumanMessage,
    SystemMessage
)

text = """Mojo is a programming language in the Python family that is currently under development.[2][3][4] It is available both in browsers via Jupyter notebooks,[4][5] and locally on Linux and macOS.[6][7] Mojo aims to combine the usability of higher level programming languages, specifically Python, with the performance of lower level programming languages like C++, Rust, and Zig.[8] The Mojo compiler is currently closed source with an open source standard library, although Modular, the company behind Mojo, has stated their intent to eventually open source the Mojo programming language itself as it matures.[9]

Mojo builds upon the MLIR compiler framework instead of directly on the lower level LLVM compiler framework that many languages like Julia, Swift, clang and Rust do.[10][11] MLIR is a newer compiler framework that allows Mojo to take advantage of higher level compiler passes not available in LLVM alone and allows Mojo to compile down and target more than just CPUs, including producing code that can run on GPUs, TPUs, ASICs and other accelerators. It can also often more effectively use certain types of CPU optimizations directly, like SIMD without direct intervention by the developer like in many other languages.[12][13] According to Jeremy Howard of fast.ai, Mojo can be seen as "syntax sugar for MLIR" and for that reason Mojo is well optimized for applications like AI."""


messages = [
    SystemMessage(content='You are an expert copywriter with expertise in summarizing documents'),
    HumanMessage(content=f'Please provide a short and concise summary of following text:\n TEXT: {text}'),
]

llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')

summary_output = llm.invoke(messages)

summary_output.content

"""# **B) Summarizing Using Prompt Templates**

For basic prompt, prompt template and StuffDocumentChain the text to be summarized and the summary should not exceed the token limit of the model.
"""

from langchain import PromptTemplate
from langchain.chains import LLMChain

template = '''
Write a concise and short summary of following text:
Text: {text}
Translate summary to {language}.
'''

prompt = PromptTemplate(
    input_variables = ['text','language'],
    template = template
)

llm.get_num_tokens(prompt.format(text=text, language='English'))

chain = LLMChain(llm=llm, prompt=prompt)
summary_output = chain.invoke({'text':text,'language':'Hindi'})

summary_output['text']

"""# **C) Summarizing With StuffDocumentChain**"""

from langchain import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.chains.summarize import load_summarize_chain
from langchain.docstore.document import Document

with open('sj.txt') as f:
  text = f.read()

# text

docs = [Document(page_content=text)]
llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')

template = '''Write a short and concise summary of following text
TEXT: '{text}'
'''

prompt = PromptTemplate(
    input_variables=['text'],
    template=template
)

chain = load_summarize_chain(
    llm,
    chain_type='stuff',
    prompt=prompt,
    verbose=False
)

output = chain.invoke(docs)
output['output_text']

"""# **D) Summarizing Large Documents using mapReduce**

This method will split the document into small chunks that will fit the token limit of the model. Then it will first summarize each chunk and then summarize the the summaries.

map_reduce uses 2 prompts to summarize the text one for chunks and then for overall summaries.

CON: 1. Some Information might be lost in between
     2. Many calls to LLM, not suitable with the free OPENAI plan.
"""

from langchain import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.chains.summarize import load_summarize_chain
from langchain.text_splitter import RecursiveCharacterTextSplitter

with open('sj.txt') as f:
  text = f.read()

# text

llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')

text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=50)

chunks = text_splitter.create_documents([text])

len(chunks)

chain = load_summarize_chain(
    llm,
    chain_type = 'map_reduce',
    verbose=False
)

output_summary = chain.invoke(chunks)

output_summary['output_text']

# The first prompt to summarize the chunks
chain.llm_chain.prompt.template

# The second prompt to summarize the summaries
chain.combine_document_chain.llm_chain.prompt.template

"""# **E) map_reduce with Custom Prompts**"""

map_prompt = '''Write a concise and short summary of the following:\n
TEXT: {text} \n
CONCISE SUMMARY:
'''

map_prompt_template = PromptTemplate(
    input_variables=['text'],
    template = map_prompt
)

combine_prompt = '''
Write a concise and short summary of the following text that covers the key points.
Add a title to the summary.
Start your summary with the INTRODUCTION PARAGRAPH that gives an overview of the topic FOLLOWED
by BULLET POINTS if possible AND end with the SUMMARY OF CONCLUSION PHRASE.
TEXT: {text}
'''

combine_prompt_template = PromptTemplate(
    input_variables = ['text'],
    template=combine_prompt
)

chain = load_summarize_chain(
    llm=llm,
    chain_type='map_reduce',
    map_prompt = map_prompt_template,
    combine_prompt = combine_prompt_template,
    verbose=False
)

output_summary = chain.invoke(chunks)

output_summary

"""# **F) Summarization Using refine chain**

In this the document is split into chunks as well.

STEP 1
summarize(chunk #1) => summary #1
STEP 2
summarize(summary #1 + chunk #2) => summary #2
STEP 3
summarize(summary #2 + chunk #3) => summary #3

...
STEP n
summarize(summary #n-1 + chunk #n) => summary #n

Pros:
1. uses a more relevant context(better summarization)
2. less lossy than map_reduce

Cons:
1. it requires many more calls to LLM
2. the calls are not independent and cannot be parallelized.

"""

from langchain import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain.chains.summarize import load_summarize_chain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader('attention_is_all_you_need.pdf')
data = loader.load()

data[0].page_content

text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=100)
chunks = text_splitter.split_documents(data)

llm=ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')

chain = load_summarize_chain(
    llm=llm,
    chain_type='refine',
    verbose=False
)

output_summary = chain.invoke(chunks)

output_summary['output_text']

"""# **G) refine With Custom Prompts**"""

prompt_template = '''Write a concise summary of the following extracting the key information:
Text: {text}
CONCISE SUMMARY:"""
'''

initial_prompt = PromptTemplate(template=prompt_template, input_variables=['text'])

refine_template = '''
Your job is to produce a final summary.
I have provided an existing summary up to a certain point: {existing_answer}.
Please refine the existing summary with some more context below.
------------
{text}
------------
Start the final summary with an INTRODUCTION PARAGRAPH that gives an overview of the topic FOLLOWED by BULLET POINTS if possible
AND end the summary with a CONCLUSION PHRASE.
'''

refine_prompt = PromptTemplate(template=refine_template, input_variables=['existing_answer','text'])

chain= load_summarize_chain(
    llm,
    chain_type='refine',
    question_prompt=initial_prompt,
    refine_prompt=refine_prompt,
    return_intermediate_steps=False
)

output_summary = chain.invoke(chunks)

print(output_summary['output_text'])

"""# **H) Summarizing Using LangChain Agents**"""

from langchain_openai import ChatOpenAI
from langchain.agents import initialize_agent, Tool
from langchain.utilities import WikipediaAPIWrapper

llm=ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')
wikipedia = WikipediaAPIWrapper()

tools = [
    Tool(
        name='Wikipedia',
        func=wikipedia.run,
        description='Useful when we need to get the information from wikipedia about a topic'
    )
]

agent_executor = initialize_agent(tools, llm, agent='zero-shot-react-description', verbose=True)

output_summary = agent_executor.invoke('Can you please provide a summary on George Washington?')

print(output_summary['output'])